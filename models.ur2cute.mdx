



# <kbd>module</kbd> `neuralforecast.models.ur2cute`
UR2CUTE: Using Repetitively 2 CNNs for Unsteady Timeseries Estimation 

A dual CNN approach for intermittent time series forecasting that combines: 1. A CNN-based classification model to predict demand occurrence (zero vs. non-zero) 2. A CNN-based regression model to estimate the magnitude of demand 

This two-step hurdle approach significantly improves forecasting accuracy for intermittent demand patterns characterized by periods of zero demand followed by random non-zero demand. 

References: 
----------- Mirshahi, S., Brandtner, P., & Komínková Oplatková, Z. (2024). Intermittent Time Series Demand Forecasting Using Dual Convolutional Neural Networks. MENDEL — Soft Computing Journal, 30(1). 



---



## <kbd>class</kbd> `CNNClassifier`
PyTorch CNN model for classification (zero vs. nonzero demand occurrence) 

Parameters 
---------- n_features : int  Number of input features forecast_horizon : int  Number of future steps to predict dropout_rate : float, optional (default=0.4)  Dropout rate for regularization 



### <kbd>method</kbd> `__init__`

```python
__init__(n_features, forecast_horizon, dropout_rate=0.4)
```








---



### <kbd>method</kbd> `forward`

```python
forward(x)
```

Forward pass for classification 

Parameters 
---------- x : torch.Tensor  Input tensor of shape [batch, features, 1] 

Returns 
------- torch.Tensor  Probability of non-zero demand for each horizon step [batch, horizon] 


---



## <kbd>class</kbd> `CNNRegressor`
PyTorch CNN model for regression (demand magnitude estimation) 

Parameters 
---------- n_features : int  Number of input features forecast_horizon : int  Number of future steps to predict dropout_rate : float, optional (default=0.2)  Dropout rate for regularization 



### <kbd>method</kbd> `__init__`

```python
__init__(n_features, forecast_horizon, dropout_rate=0.2)
```








---



### <kbd>method</kbd> `forward`

```python
forward(x)
```

Forward pass for regression 

Parameters 
---------- x : torch.Tensor  Input tensor of shape [batch, features, 1] 

Returns 
------- torch.Tensor  Predicted demand magnitude for each horizon step [batch, horizon] 


---



## <kbd>class</kbd> `UR2CUTE`
UR2CUTE: Using Repetitively 2 CNNs for Unsteady Timeseries Estimation 

A two-step hurdle approach for intermittent demand forecasting: 1. Classification CNN predicts whether demand will occur (zero vs. non-zero) 2. Regression CNN predicts the magnitude of demand 

The final forecast combines both models: if the classification probability exceeds the threshold, the regression output is used; otherwise, zero is predicted. 

Parameters 
---------- h : int  Forecast horizon - number of future steps to predict input_size : int  Number of historical timesteps to use as input (lookback window) loss : pytorch module, optional (default=MAE())  Training loss function. For the two-step approach, this primarily affects  the regression model's training. valid_loss : pytorch module, optional (default=None)  Validation loss function. If None, uses the same as loss. classification_threshold : float or str, optional (default=0.5)  Probability threshold for classifying zero vs. non-zero demand.  Can be a float between 0 and 1, or "auto" to compute from training data. dropout_classification : float, optional (default=0.4)  Dropout rate for the classification CNN dropout_regression : float, optional (default=0.2)  Dropout rate for the regression CNN classification_weight : float, optional (default=0.3)  Weight for classification loss in combined loss (between 0 and 1).  Total loss = classification_weight * BCE + (1 - classification_weight) * regression_loss learning_rate : float, optional (default=1e-3)  Learning rate for optimization max_steps : int, optional (default=1000)  Maximum number of training steps val_check_steps : int, optional (default=100)  Validation check frequency batch_size : int, optional (default=32)  Batch size for training random_seed : int, optional (default=1)  Random seed for reproducibility **trainer_kwargs : additional arguments  Additional arguments passed to PyTorch Lightning Trainer 

Examples 
-------- ``` from neuralforecast import NeuralForecast```
``` from neuralforecast.models import UR2CUTE``` ``` from neuralforecast.losses.pytorch import MAE```
`````` ``` # Create model for intermittent demand forecasting```
``` model = UR2CUTE(``` ...     h=12,                              # Forecast 12 periods ahead ...     input_size=24,                     # Use last 24 periods as input ...     classification_threshold=0.5,       # 50% probability threshold ...     dropout_classification=0.4, ...     dropout_regression=0.2, ...     max_steps=1000, ...     learning_rate=0.001 ... ) ``````
``` # Initialize NeuralForecast with the model``` ``` nf = NeuralForecast(models=[model], freq='W')```
`````` ``` # Fit on training data```
``` nf.fit(df=train_df)``` ``````
``` # Make predictions``` ``` forecasts = nf.predict(df=test_df)```

References

----------
Mirshahi, S., Brandtner, P., & Komínková Oplatková, Z. (2024).
Intermittent Time Series Demand Forecasting Using Dual Convolutional Neural Networks.
MENDEL — Soft Computing Journal, 30(1).




### <kbd>method</kbd> `__init__`

```python
__init__(
    h: int,
    input_size: int = -1,
    loss=MAE(),
    valid_loss=None,
    classification_threshold: float = 0.5,
    dropout_classification: float = 0.4,
    dropout_regression: float = 0.2,
    classification_weight: float = 0.3,
    learning_rate: float = 0.001,
    max_steps: int = 1000,
    val_check_steps: int = 100,
    batch_size: int = 32,
    random_seed: int = 1,
    **trainer_kwargs
)
```






---

#### <kbd>property</kbd> automatic_optimization

If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``. 

---

#### <kbd>property</kbd> current_epoch

The current epoch in the ``Trainer``, or 0 if not attached. 

---

#### <kbd>property</kbd> device





---

#### <kbd>property</kbd> device_mesh

Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the :meth:`~pytorch_lightning.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule. 

---

#### <kbd>property</kbd> dtype





---

#### <kbd>property</kbd> example_input_array

The example input array is a specification of what the module can consume in the :meth:`forward` method. The return type is interpreted as follows: 


-   Single tensor: It is assumed the model takes a single argument, i.e.,  ``model.forward(model.example_input_array)`` 
-   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,  ``model.forward(*model.example_input_array)`` 
-   Dict: The input array represents named keyword arguments, i.e.,  ``model.forward(**model.example_input_array)`` 

---

#### <kbd>property</kbd> fabric





---

#### <kbd>property</kbd> global_rank

The index of the current process across all nodes and devices. 

---

#### <kbd>property</kbd> global_step

Total training batches seen across all epochs. 

If no Trainer is attached, this property is 0. 

---

#### <kbd>property</kbd> hparams

The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For the frozen set of initial hyperparameters, use :attr:`hparams_initial`. 



**Returns:**
  Mutable hyperparameters dictionary 

---

#### <kbd>property</kbd> hparams_initial

The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`. 



**Returns:**
 
 - <b>`AttributeDict`</b>:  immutable initial hyperparameters 

---

#### <kbd>property</kbd> local_rank

The index of the current process within a single node. 

---

#### <kbd>property</kbd> logger

Reference to the logger object in the Trainer. 

---

#### <kbd>property</kbd> loggers

Reference to the list of loggers in the Trainer. 

---

#### <kbd>property</kbd> on_gpu

Returns ``True`` if this model is currently located on a GPU. 

Useful to set flags around the LightningModule for different CPU vs GPU behavior. 

---

#### <kbd>property</kbd> strict_loading

Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`. 

---

#### <kbd>property</kbd> trainer







---



### <kbd>method</kbd> `forward`

```python
forward(windows_batch)
```

Forward pass implementing the two-step hurdle approach 

Parameters 
---------- windows_batch : dict  Dictionary containing: 
    - insample_y : torch.Tensor [Batch, input_size, 1]  Historical target values (normalized) 
    - insample_mask : torch.Tensor [Batch, input_size, 1]  Availability mask for historical data 

Returns 
------- dict  Dictionary containing: 
    - 'forecast': torch.Tensor [Batch, h, 1]  Combined forecast (classification * regression) 
    - 'classification': torch.Tensor [Batch, h]  Raw classification probabilities 
    - 'regression': torch.Tensor [Batch, h]  Raw regression predictions 

---



### <kbd>method</kbd> `training_step`

```python
training_step(batch, batch_idx)
```

Custom training step to handle the two-step loss calculation 

Overrides BaseModel.training_step to compute combined loss: 
- BCE loss for classification (zero vs. non-zero) 
- Regression loss for magnitude prediction 

Parameters 
---------- batch : dict  Batch of data from DataLoader batch_idx : int  Index of the batch 

Returns 
------- torch.Tensor  Combined loss value 

---



### <kbd>method</kbd> `validation_step`

```python
validation_step(batch, batch_idx)
```

Custom validation step matching the training step logic 

Parameters 
---------- batch : dict  Batch of validation data batch_idx : int  Index of the batch 

Returns 
------- torch.Tensor  Combined validation loss 


